# ü§ñ LLM Integration Guide - Dynamic AI Responses

## Overview

The AIHolidayPlanner now uses **real LLM APIs** for dynamic, intelligent responses instead of hardcoded values. This provides:

- ‚úÖ **Dynamic destination suggestions** based on user preferences
- ‚úÖ **Personalized AI responses** tailored to each request
- ‚úÖ **No more hardcoded Canc√∫n defaults**
- ‚úÖ **Intelligent fallbacks** when LLM is unavailable
- ‚úÖ **Free tier available** (Groq API)

---

## üöÄ Quick Setup (5 minutes)

### Step 1: Get Free API Key

**Option A: Groq (Recommended - Fastest & Free)**
1. Go to: https://console.groq.com/
2. Sign up for free account
3. Navigate to "API Keys"
4. Click "Create API Key"
5. Copy the key (starts with `gsk_...`)

**Option B: Together AI (Alternative)**
1. Go to: https://api.together.xyz/
2. Sign up for free account
3. Get API key from dashboard
4. $25 free credit

**Option C: Hugging Face (No key needed, rate limited)**
- Works without API key
- Rate limited to ~1000 requests/day
- Slower response times

### Step 2: Configure Environment

1. Create `.env` file in project root:
   ```bash
   cp .env.example .env
   ```

2. Add your API key:
   ```bash
   VITE_LLM_API_KEY=gsk_your_actual_key_here
   ```

3. Restart development server:
   ```bash
   # Stop current server (Ctrl+C)
   npm run dev
   ```

### Step 3: Test It!

1. Open app: http://localhost:5000
2. Enter vacation request: "Adventure trip to mountains, $3000, 5 days"
3. Watch for dynamic AI response (not hardcoded!)
4. Check console for: `LLM response generated`

---

## üéØ What Changed

### Before (Hardcoded):
```typescript
// Always suggested Canc√∫n
destination: destinations[0]  // ‚ùå Always Canc√∫n

// Hardcoded responses
"Planning your trip to Canc√∫n..."  // ‚ùå Static
```

### After (Dynamic):
```typescript
// Intelligent destination matching
- Analyzes user description
- Matches keywords (beach, mountain, culture)
- Uses LLM for suggestions
- Random fallback (not always Canc√∫n)

// Dynamic AI responses
- Generated by LLM based on actual preferences
- Personalized to budget, duration, interests
- Mentions specific details from user input
```

---

## üìä Features

### 1. Dynamic Destination Resolution

**Old Behavior:**
- No destination mentioned ‚Üí Always Canc√∫n ‚ùå
- Vague description ‚Üí Always Canc√∫n ‚ùå

**New Behavior:**
- Beach keywords ‚Üí Random beach destination ‚úÖ
- Culture keywords ‚Üí Cultural destination ‚úÖ
- Adventure keywords ‚Üí Adventure destination ‚úÖ
- No keywords ‚Üí Random destination (not always Canc√∫n) ‚úÖ

### 2. LLM-Powered Suggestions

When LLM is available:
```typescript
// User: "Beach vacation, $5000, 7 days"
// LLM suggests:
[
  { name: "Maldives", matchScore: 95, reasons: [...] },
  { name: "Bali", matchScore: 92, reasons: [...] },
  { name: "Seychelles", matchScore: 90, reasons: [...] }
]
```

### 3. Personalized AI Responses

**Old (Template):**
```
"I've curated a luxury itinerary for Canc√∫n. Your 7-day journey..."
```

**New (LLM-Generated):**
```
"Fantastic! I'm planning your 7-day beach adventure to Maldives 
with a $5,000 budget. Perfect for couples seeking relaxation and 
water sports. Our AI team is now finding the best overwater villas 
and diving experiences!"
```

### 4. Intelligent Fallbacks

If LLM fails or no API key:
- ‚úÖ Uses smart template system
- ‚úÖ Analyzes keywords for destination matching
- ‚úÖ Randomizes suggestions (no more Canc√∫n bias)
- ‚úÖ Still provides good user experience

---

## üîß Configuration Options

### Change LLM Provider

Edit `client/src/lib/llm-service.ts`:

```typescript
// Use Groq (default - fastest)
llmService.setProvider('groq');

// Use Together AI
llmService.setProvider('together');

// Use Hugging Face (no key needed)
llmService.setProvider('huggingface');
```

### Adjust LLM Parameters

In `llm-service.ts`:

```typescript
{
  model: config.model,
  messages,
  temperature: 0.7,  // 0.0 = deterministic, 1.0 = creative
  max_tokens: 1000,  // Response length
}
```

---

## üß™ Testing

### Test 1: Dynamic Destination Matching

```bash
# Test beach keywords
Input: "Beach vacation with snorkeling"
Expected: Suggests beach destinations (Bali, Maldives, etc.)

# Test culture keywords
Input: "Historical sites and museums"
Expected: Suggests cultural destinations (Rome, Athens, etc.)

# Test adventure keywords
Input: "Hiking and mountain climbing"
Expected: Suggests adventure destinations (Nepal, Peru, etc.)
```

### Test 2: LLM Response Generation

```bash
# With API key
1. Set VITE_LLM_API_KEY in .env
2. Enter vacation request
3. Check console for: "LLM response generated"
4. Verify response mentions your specific details

# Without API key
1. Remove VITE_LLM_API_KEY from .env
2. Enter vacation request
3. Should still work with template fallback
4. Check console for: "LLM not available, using template"
```

### Test 3: No More Canc√∫n Bias

```bash
# Test 10 times with vague input
for i in {1..10}; do
  Input: "Nice vacation somewhere warm"
  Record: Which destination was suggested?
done

Expected: Different destinations each time
Not Expected: Canc√∫n every time
```

---

## üìà Performance

### With LLM (Groq):
- Response time: ~500ms
- Quality: Excellent, personalized
- Cost: Free tier (30 requests/min)

### With LLM (Together AI):
- Response time: ~1000ms
- Quality: Excellent
- Cost: $25 free credit

### Without LLM (Fallback):
- Response time: <10ms
- Quality: Good, template-based
- Cost: Free

---

## üîí Security

### API Key Protection:
- ‚úÖ Stored in `.env` (not committed to git)
- ‚úÖ Accessed via `import.meta.env`
- ‚úÖ Never exposed to client-side logs
- ‚úÖ Can be rotated anytime

### Rate Limiting:
- ‚úÖ Groq: 30 requests/min (free tier)
- ‚úÖ Together: Based on credit
- ‚úÖ Hugging Face: ~1000/day

---

## üêõ Troubleshooting

### Issue: "API key required"
**Solution:**
```bash
# Check .env file exists
ls -la .env

# Check key is set
cat .env | grep VITE_LLM_API_KEY

# Restart dev server
npm run dev
```

### Issue: "LLM API error: 401"
**Solution:**
- API key is invalid
- Get new key from provider
- Update `.env` file
- Restart server

### Issue: "LLM API error: 429"
**Solution:**
- Rate limit exceeded
- Wait a few minutes
- Or switch to different provider

### Issue: Responses still seem hardcoded
**Solution:**
```bash
# Check LLM is being used
1. Open browser console
2. Look for: "LLM response generated"
3. If not found, check API key setup
4. Verify .env file is loaded
```

---

## üí° Tips

### For Development:
1. Use Groq for fastest responses
2. Keep API key in `.env` (not `.env.example`)
3. Monitor console for LLM status
4. Test with and without API key

### For Production:
1. Use environment variables (not `.env` file)
2. Set up error tracking (Sentry)
3. Monitor API usage
4. Have fallback ready

### For Testing:
1. Try various destination types
2. Test with different budgets
3. Vary duration and interests
4. Check response personalization

---

## üìö API Documentation

### Groq API:
- Docs: https://console.groq.com/docs
- Models: Llama 3.1, Mixtral, Gemma
- Speed: ~500ms response time
- Free tier: 30 req/min

### Together AI:
- Docs: https://docs.together.ai/
- Models: 50+ open source models
- Speed: ~1000ms response time
- Free: $25 credit

### Hugging Face:
- Docs: https://huggingface.co/docs/api-inference
- Models: 100,000+ models
- Speed: Variable (2-5s)
- Free: Rate limited

---

## üéØ Success Criteria

Your LLM integration is working when:

1. ‚úÖ Different destinations suggested for same vague input
2. ‚úÖ AI responses mention specific user details
3. ‚úÖ Console shows "LLM response generated"
4. ‚úÖ No more "always Canc√∫n" behavior
5. ‚úÖ Responses feel personalized
6. ‚úÖ Fallback works without API key

---

## üöÄ Next Steps

### Immediate:
1. Get Groq API key (free)
2. Add to `.env` file
3. Restart server
4. Test with various inputs

### Future Enhancements:
1. Add more LLM providers
2. Implement caching for common queries
3. Add user feedback loop
4. Fine-tune prompts for better results
5. Add streaming responses

---

## üìû Support

### Getting API Keys:
- Groq: https://console.groq.com/keys
- Together: https://api.together.xyz/settings/api-keys
- Hugging Face: https://huggingface.co/settings/tokens

### Issues:
1. Check console for errors
2. Verify API key is correct
3. Test fallback mode (remove API key)
4. Check network tab for API calls

---

**Implementation Date**: November 14, 2025  
**Status**: ‚úÖ Fully Implemented  
**LLM Provider**: Groq (recommended)  
**Fallback**: Smart templates  
**Cost**: Free tier available
